{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "install and run the repo, make sure to connect with the GPU instance to run inference fast!"
      ],
      "metadata": {
        "id": "GiV3k8UHvUvO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pXlhbrutKmN",
        "outputId": "0454b65a-226f-4023-871a-f551484385fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'QALLM'...\n",
            "remote: Enumerating objects: 200, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 200 (delta 10), reused 16 (delta 5), pack-reused 178\u001b[K\n",
            "Receiving objects: 100% (200/200), 688.80 KiB | 14.97 MiB/s, done.\n",
            "Resolving deltas: 100% (107/107), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/faizan1234567/QALLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHIlvYS-tZab",
        "outputId": "8a2535eb-2124-4480-d1ca-ac56aa721d61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mQALLM\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd QALLM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVEPYwrTtbTU",
        "outputId": "8180613c-3e0a-4121-fe82-100d4805f13c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/QALLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install dependencies\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "#if required then restart the session to install all the depencies\n",
        "# it is recommned to cd again to working directory"
      ],
      "metadata": {
        "id": "MgV7BDr5td4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd QALLM/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_8gpyNstr3u",
        "outputId": "1b6063a0-82ae-4c2e-a2b1-27ce716d056e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/QALLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUJcrmEKuVXf",
        "outputId": "636c8832-d69b-43d6-cb8b-4ab0c63a01f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcfg\u001b[0m/                             \u001b[01;34mdataset\u001b[0m/  LICENSE     qa_generator.py  requirements.txt\n",
            "create_alpaca_format_dataset.py  \u001b[01;34mimages\u001b[0m/   \u001b[01;34mnotebooks\u001b[0m/  README.md        utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test if everything runs smoothly\n",
        "!python qa_generator.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_jFzuqluWAi",
        "outputId": "33deea59-d634-4254-eb0d-879b1392dc85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: qa_generator.py [-h] [--model {GPT3.5-Turbo,GPT-4,Llama2,T5-small}]\n",
            "                       [--cfg {cfg/qa_generator.yaml,cfg/fine_tuning.yaml}]\n",
            "\n",
            "Command Line arguments\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model {GPT3.5-Turbo,GPT-4,Llama2,T5-small}\n",
            "                        name of the model\n",
            "  --cfg {cfg/qa_generator.yaml,cfg/fine_tuning.yaml}\n",
            "                        configuration file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "GObof5VvvB2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run T5-small\n",
        "!python qa_generator.py --model \"T5-small\" --cfg cfg/qa_generator.yaml\n"
      ],
      "metadata": {
        "id": "E1APKaJ4ufFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run Llama2 (not recommended as it is too slow)\n",
        "!python qa_generator.py --model \"Llama2\" --cfg cfg/qa_generator.yaml"
      ],
      "metadata": {
        "id": "V2lmBtmwuuUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try with GPT4 or GPT3.5 Turbo run: make sure you have the api key (recommended for QA generation)\n",
        "!python qa_generator.py --model \"GPT-3.5Turbo\" --cfg cfg/qa_generator.yaml"
      ],
      "metadata": {
        "id": "xA9WQCS8x0en"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}